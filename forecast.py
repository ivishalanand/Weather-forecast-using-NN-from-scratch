# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C4uPhE7ewQyuFb2Q4T27izmRxV8KbL7K
"""

#Load the csv file as data frame.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

df=read_csv("our_data")

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv(io.BytesIO(uploaded['weatherAUS.csv']))
# Dataset is now stored in a Pandas Dataframe

print('Size of weather data frame is :',df.shape)

df[0:5]

# We see there are some columns with null values. 
# Before we start pre-processing, let's find out which of the columns have maximum null values
df.count().sort_values()

# As we can see the first four columns have less than 60% data, we can ignore these four columns
# We don't need the location column because 
# we are going to find if it will rain 
# We are going to drop the date column too.
# We need to remove RISK_MM because we want to predict 'RainTomorrow' and RISK_MM can leak some info to our model
df = df.drop(columns=['Sunshine','Evaporation','Cloud3pm','Cloud9am','Location','RISK_MM','Date'],axis=1)
df.shape

#Let us get rid of all null values in df
df = df.dropna(how='any')
df.shape

#Dealing with the categorical cloumns now
# simply changing yes/no to 1/0 for RainToday and RainTomorrow
df['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)
df['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)

#See unique values and convert them to int using pd.getDummies()
categorical_columns = ['WindGustDir', 'WindDir3pm', 'WindDir9am']
for col in categorical_columns:
    print(np.unique(df[col]))

# transform the categorical columns
df = pd.get_dummies(df, columns=categorical_columns)
df.iloc[4:9]



#next step is to standardize our data - using MinMaxScaler
#X_scaled = X - X_min / X_max-X_min
from sklearn import preprocessing
scaler = preprocessing.MinMaxScaler()
scaler.fit(df)
df = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.columns)
df.iloc[4:10]

df.to_csv

X = df.loc[:,df.columns!='RainTomorrow']
  y = df[['RainTomorrow']]

from google.colab import files

df.to_csv('df.csv')
files.download('df.csv')

X_arr=np.array(X)
y_arr=np.array(y)



df.info()

#Training and running the model
from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import accuracy_score

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)
clf_logreg = LogisticRegression(random_state=0)
clf_logreg.fit(X_train,y_train)
y_pred = clf_logreg.predict(X_test)
score = accuracy_score(y_test,y_pred)
print('Accuracy :',score)

def hypothesis(theta, X, n):
    h = np.ones((X.shape[0],1))
    theta = theta.reshape(1,n+1)
    for i in range(0,X.shape[0]):
        h[i] = 1 / (1 + np.exp(-float(np.matmul(theta, X[i]))))
    h = h.reshape(X.shape[0])
    return h
  
def BGD(theta, alpha, num_iters, h, X, y, n):
    theta_history = np.ones((num_iters,n+1))
    cost = np.ones(num_iters)
    for i in range(0,num_iters):
        theta[0] = theta[0] - (alpha/X.shape[0]) * sum(h - y)
        for j in range(1,n+1):
            theta[j]=theta[j]-(alpha/X.shape[0])*sum((h-y)
                               *X.transpose()[j])
        theta_history[i] = theta
        h = hypothesis(theta, X, n)
        cost[i]=(-1/X.shape[0])*sum(y*np.log(h)+(1-y)*np.log(1 - h))
    theta = theta.reshape(1,n+1)
    return theta, theta_history, cost
  
def logistic_regression(X, y, alpha, num_iters):
    n = X.shape[1]
    one_column = np.ones((X.shape[0],1))
    X = np.concatenate((one_column, X), axis = 1)
    # initializing the parameter vector...
    theta = np.zeros(n+1)
    # hypothesis calculation....
    h = hypothesis(theta, X, n)
    # returning the optimized parameters by Gradient Descent...
    theta,theta_history,cost = BGD(theta,alpha,num_iters,h,X,y,n)
    return theta, theta_history, cost

# -*- coding: utf-8 -*-
"""
Created on Thu Jul 25 16:56:28 2019

@author: vishal
"""

import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt

array = [[93,	3,	8],
[0,188,41],
[0,25,284]]


        
df_cm = pd.DataFrame(array, ["      Type 1 Pred","Type 2 Pred","Type 3 Pred"],["Type 1 Actual","Type 2 Actual","Type 3 Actual"])
#plt.figure(figsize = (10,7))
sn.set(font_scale=1.4)#for label size
fig, ax = plt.subplots(figsize=(10,6))
sn.heatmap(df_cm,annot=True,annot_kws={"size": 20,"weight":"bold"},fmt='g')

import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt
array = [[33,2,0,0,0,0,0,0,0,1,3], 
        [3,31,0,0,0,0,0,0,0,0,0], 
        [0,4,41,0,0,0,0,0,0,0,1], 
        [0,1,0,30,0,6,0,0,0,0,1], 
        [0,0,0,0,38,10,0,0,0,0,0], 
        [0,0,0,3,1,39,0,0,0,0,4], 
        [0,2,2,0,4,1,31,0,0,0,2],
        [0,1,0,0,0,0,0,365,0,2,0], 
        [0,0,0,0,0,0,1,5,37,5,1], 
        [3,0,0,0,0,0,0,0,0,39,0], 
        [0,0,0,0,0,0,0,0,0,0,38]]
df_cm = pd.DataFrame(array, index = [i for i in "ABCDEFGHIJK"],
                  columns = [i for i in "ABCDEFGHIJK"])
plt.figure(figsize = (10,7))
sn.heatmap(df_cm, annot=True)

X_new= X_arr[1:100,:]
y_new=y_arr[1:100]

y_new

theta,theta_history,cost=logistic_regression(X_train,y_train,0.001,1)

2+3

df.info()

import numpy as np                                           #importing numpy library
import pandas as pd                                          #importing pandas library
import matplotlib.pyplot as plt

data  = pd.read_csv("df.csv")

data.drop('Unnamed: 0',inplace = True,axis = 1)

y = data['RainTomorrow']

X = data.drop(['RainTomorrow'],inplace = False,axis = 1)

length = data.shape[0]
train = int(data.shape[0]*0.6)
X_train = np.array(X)[0:train]
y_train = np.array(y)[0:train]
X_test = np.array(X)[train:length]
y_test = np.array(y)[train:length]

class NeuralNetwork:                                  #creating Neural Network class
    
    def __init__(self,X, y, X_test, y_test, hidden_nodes=12, learning_rate=0.1, epochs=5000):
        
        #data
        self.y = y[:,None]
        self.X = X
        
        self.X_test = X_test
        self.y_test = y_test
        
        #parameters
        np.random.seed(4)
        self.input_nodes = len(X[0])
        self.hidden_nodes = hidden_nodes
        self.output_nodes = self.y.shape[1]
        self.learning_rate = learning_rate
        
        #init weights
        self.w1 = 2*np.random.random((self.input_nodes, self.hidden_nodes)) - 1
        self.w2 = 2*np.random.random((self.hidden_nodes, self.output_nodes)) - 1

        self.train(epochs)
        self.test()
        
    def sigmoid(self,X):
        return (1/(1+np.exp(-X)))

    def sigmoid_prime(self,X):
        return X * (1 - X)
        
    def train(self, epochs):
        
        for e in range(epochs):
            
            #FORWARD PROPAGATION
            
            #hidden layer
            # W1(398,30) X(30,12)
            l1 = self.sigmoid(np.dot(self.X, self.w1))

            #output layer
            #l1(398,12) W2(12,1)
            l2 = self.sigmoid(np.dot(l1, self.w2))
        
            # BACKPROPAGATION
            
            #calculate how far off our prediciton was
            error = self.y-l2
            
            #calculate how far off each layer is
            l2_delta = error * self.sigmoid_prime(l2)
            l1_delta = l2_delta.dot(self.w2.T) * self.sigmoid_prime(l1)

            #update weights with our newly found error values
            self.w2 = np.add(self.w2, l1.T.dot(l2_delta) * self.learning_rate)
            self.w1 = np.add(self.w1, self.X.T.dot(l1_delta) * self.learning_rate)
        
        print('Error:', (abs(error)).mean())
    
    def test(self):
        correct = 0
        pred_list = []
        
        #replicate feedforward network for testing
        l1 = self.sigmoid(np.dot(self.X_test, self.w1))
        l2 = self.sigmoid(np.dot(l1, self.w2))
        
        #loop through all of the outputs of layer 2
        for i in range(len(l2)):
            if l2[i] >= 0.5:
                pred = 1
            else:
                pred = 0

            if pred == self.y_test[i]:
                correct += 1
                
            pred_list.append(pred)

        print("Test Accuracy: ", ((correct/len(y_test))*100),'%')
        
        #confusion matrix
        #cm = confusion_matrix(y_test, pred_list)
        #sns.heatmap(cm,annot=True)
        #plt.savefig('h.png')
        #plt.show()

nn = NeuralNetwork(X_train, y_train, X_test, y_test)

import keras

import numpy as np
docs = ['Well done!',
		'Good work',
		'Great effort',
		'nice work',
		'Excellent!',
		'Weak',
		'Poor effort!',
		'not good',
		'poor work',
		'Could have done better.']
# define class labels
labels = np.array([1,1,1,1,1,0,0,0,0,0])
import keras
# integer encode the documents
vocab_size = 50
encoded_docs = [keras.preprocessing.text.one_hot(d, vocab_size) for d in docs]
print(encoded_docs)

max_length = 4
padded_docs = keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=max_length, padding='post')
print(padded_docs)

# define the model
model = Sequential()
model.add(Embedding(vocab_size, 8, input_length=max_length))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
# summarize the model
print(model.summary())

from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
# define documents
docs = ['Well done!',
		'Good work',
		'Great effort',
		'nice work',
		'Excellent!',
		'Weak',
		'Poor effort!',
		'not good',
		'poor work',
		'Could have done better.']
# define class labels
labels = array([1,1,1,1,1,0,0,0,0,0])
# integer encode the documents
vocab_size = 50
encoded_docs = [one_hot(d, vocab_size) for d in docs]
print(encoded_docs)
# pad documents to a max length of 4 words
max_length = 4
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
print(padded_docs)
# define the model
model = Sequential()
model.add(Embedding(vocab_size, 8, input_length=max_length))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
# summarize the model
print(model.summary())
# fit the model
model.fit(padded_docs, labels, epochs=50, verbose=0)
# evaluate the model
loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)
print('Accuracy: %f' % (accuracy*100))